{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchnet as tnt\n",
    "\n",
    "from torchvision import datasets, transforms, models\n",
    "# from Places205 import Places205\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path as osp\n",
    "import sys\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from adversarial_contrastive_model import resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NT-Xent Begins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dir = '/data/majie/Datasets/TinyDataset/hymenoptera_data'\n",
    "# ant = osp.join(data_dir, 'train', 'ants', 'VietnameseAntMimicSpider.jpg')\n",
    "# bee = osp.join(data_dir, 'train', 'bees', '39747887_42df2855ee.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_image(file_path):\n",
    "#     image = Image.open(file_path).convert('RGB')\n",
    "# #     image = np.asarray(image)\n",
    "#     return image # PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform_crop = transforms.Compose([transforms.RandomCrop(224), transforms.ToTensor()])\n",
    "# transform_color = transforms.Compose([transforms.Resize((224, 224)), \n",
    "#                                       transforms.ColorJitter(1, 0.1, 0.05, 0.0), \n",
    "#                                       transforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ant_crop = transform_crop(load_image(ant))\n",
    "# ant_color = transform_color(load_image(ant)) # torch.Size([3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bee_crop = transform_crop(load_image(bee))\n",
    "# bee_color = transform_color(load_image(bee)) # torch.Size([3, 224, 224])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ant1 = ant_crop.view(1, -1)\n",
    "# ant2 = ant_color.view(1, -1)\n",
    "# bee1 = bee_crop.view(1, -1)\n",
    "# bee2 = bee_color.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ant1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2463476998f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manimals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mant1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mant2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbee1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbee2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ant1' is not defined"
     ]
    }
   ],
   "source": [
    "animals = [ant1, ant2, bee1, bee2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造 L2 distance 矩阵， 计算 每张 图片 与 所有图片的距离 N * N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1 = [F.pairwise_distance(animals[0], animals[x]) for x in range(4)]\n",
    "# s2 = [F.pairwise_distance(animals[1], animals[x]) for x in range(4)]\n",
    "# s3 = [F.pairwise_distance(animals[2], animals[x]) for x in range(4)]\n",
    "# s4 = [F.pairwise_distance(animals[3], animals[x]) for x in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = torch.Tensor([[torch.cosine_similarity(animals[i], animals[x]) for x in range(4)] for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S = torch.Tensor([[F.pairwise_distance(animals[i], animals[x]) for x in range(4) if x != i] for i in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让同一张图的不同view在latent space里靠近，不同图的view在latent space里远离，通过NT-Xent实现\n",
    "Loss12 = S[0][1] / sum(S[0])\n",
    "Loss21 = S[1][0] / sum(S[1])\n",
    "Loss34 = S[2][3] / sum(S[2])\n",
    "Loss43 = S[3][2] / sum(S[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss12 + Loss21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Loss34 + Loss43"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_distance(k):\n",
    "    # k: [1,.., N]\n",
    "    return S[2*k - 1-1][2*k-1] / torch.sum(S[2 *k-1-1]) + S[2*k - 1][2*k - 1-1] / torch.sum(S[2*k -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_distance(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L_distance(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 0\n",
    "for k in range(1, 3):\n",
    "    loss += L_distance(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss / 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply NT-Xent in multi domains**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(file_path):\n",
    "    image = Image.open(file_path).convert('RGB')\n",
    "#     image = np.asarray(image)\n",
    "    return image # PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/data/majie/Datasets/Domain-adaption/PACS/data'\n",
    "doms = ['cartoon', 'photo', 'sketch']\n",
    "doms_test = 'art_painting'\n",
    "doms_test = 0\n",
    "dom_datasets = {}\n",
    "data_sizes = {}\n",
    "image_datasets = {}\n",
    "image_datasets_test = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, dom in enumerate(doms):\n",
    "    image_datasets[dom] = {x: datasets.ImageFolder(os.path.join(data_dir, dom, x)) for x in ['train', 'val', 'test']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dom_dataset = datasets.ImageFolder(osp.join(data_dir, 'art_painting', 'test'), transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset ImageFolder\n",
       "    Number of datapoints: 2048\n",
       "    Root location: /data/majie/Datasets/Domain-adaption/PACS/data/art_painting/test\n",
       "    StandardTransform\n",
       "Transform: Compose(\n",
       "               CenterCrop(size=(224, 224))\n",
       "               ToTensor()\n",
       "               Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "           )"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dom_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dom_dataloader = DataLoader(test_dom_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_dom_dataloader.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = test_data[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight 64 3 7 7, but got 3-dimensional input of size [3, 224, 224] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-d5a21d5efab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_ft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/majie/Code/Fair-noise-as-targets/adversarial_contrastive_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, alpha, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight 64 3 7 7, but got 3-dimensional input of size [3, 224, 224] instead"
     ]
    }
   ],
   "source": [
    "out = model_ft(inputs, 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "  )\n",
       "  (domain_classifier): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=100, bias=True)\n",
       "    (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=100, out_features=3, bias=True)\n",
       "  )\n",
       "  (class_classifier): Linear(in_features=512, out_features=7, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ft = resnet18()\n",
    "model_ft = model_ft.to(device)\n",
    "model_ft.load_state_dict(torch.load('./models/best_model_adversarial_contrastive.tar')['state'])\n",
    "model_ft.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = 0\n",
    "# for images, labels in test_dom_dataloader:\n",
    "#     images = images.to(device)\n",
    "#     labels = labels.to(device)\n",
    "    \n",
    "#     outputs = model_ft(images, 0.8)[1]\n",
    "#     _, preds_classes = torch.max(outputs, 1)\n",
    "#     print('preds_classes', preds_classes)\n",
    "#     acc += torch.sum(preds_classes == labels.data)\n",
    "# print('test on art domain, accuracy is: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/data/majie/Datasets/Domain-adaption/PACS/data/cartoon/train/6/pic_111.jpg',\n",
       "       '6', '0'], dtype='<U84')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, val = [], []\n",
    "for idx, dom in enumerate(doms):\n",
    "    train_label = [image_datasets[dom]['train'].imgs[i] + (idx,) for i in range(len(image_datasets[dom]['train'].imgs))]\n",
    "    train.append(train_label)\n",
    "    val_label = [image_datasets[dom]['val'].imgs[i] + (idx,) for i in range(len(image_datasets[dom]['val'].imgs))]\n",
    "    val.append(val_label)\n",
    "train = np.vstack(np.array(train))\n",
    "val = np.vstack(np.array(val))\n",
    "train[1839]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericDataset(Dataset):\n",
    "    def __init__(self, dataset_root='', stage='train', random_sized_crop=False, num_imgs_per_cat=None):\n",
    "        super(GenericDataset, self).__init__()\n",
    "        \n",
    "        # if split is not None, return the spcific split data\n",
    "        # else return all data in source domains\n",
    "        \n",
    "        self.image_file_names = []\n",
    "        self.stage = stage\n",
    "        \n",
    "        self.dataset = {'train': [], 'val': []}\n",
    "        self.data = []\n",
    "        \n",
    "        if self.stage == 'test': # test domain\n",
    "            \n",
    "            dom = doms[doms_test] \n",
    "            # test split is the whole data in its domain\n",
    "            self.data.extend(image_datasets[dom]['test'].imgs)\n",
    "            \n",
    "        else: # training domain\n",
    "           \n",
    "            self.dataset['train'] = train\n",
    "            self.dataset['val'] = val\n",
    "            \n",
    "            self.data = self.dataset[self.stage]\n",
    "       \n",
    "            \n",
    "          \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 1. return img - label\n",
    "        image, class_label, dom_label = self.data[index]\n",
    "        \n",
    "        return image, class_label, dom_label\n",
    "\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = GenericDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset,batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataloader.__iter__().__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = {\n",
    "    'train': {\n",
    "        'crop': transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'color': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(1, 0.1, 0.05, 0.0), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    },\n",
    "    'val': {\n",
    "        'crop': transforms.Compose([\n",
    "            transforms.CenterCrop((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ]),\n",
    "        'color': transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ColorJitter(1, 0.1, 0.05, 0.0), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    },\n",
    "    'test': transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader_custom(object):\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 stage='train',\n",
    "                 batch_size=8,\n",
    "                 unsupervised=True,\n",
    "                 epoch_size=None,\n",
    "                 num_workers=8,\n",
    "                 shuffle=True):\n",
    "        \n",
    "        self.dataset = dataset\n",
    "        self.shuffle = shuffle\n",
    "        self.epoch_size = epoch_size if epoch_size is not None else len(dataset)\n",
    "        self.batch_size = batch_size\n",
    "        self.unsupervised = unsupervised\n",
    "        self.num_workers = num_workers\n",
    "        \n",
    "        self.stage = stage\n",
    "\n",
    "        mean_pix  = [0.485, 0.456, 0.406]\n",
    "        std_pix = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        \n",
    "        self.transform = data_transform\n",
    "     \n",
    "\n",
    "    def get_iterator(self, epoch=0):\n",
    "        rand_seed = epoch * self.epoch_size\n",
    "        random.seed(rand_seed)\n",
    "        # if in unsupervised mode define a loader function that given the\n",
    "        # index of an image it returns the 4 rotated copies of the image\n",
    "        # plus the label of the rotation, i.e., 0 for 0 degrees rotation,\n",
    "        # 1 for 90 degrees, 2 for 180 degrees, and 3 for 270 degrees.\n",
    "        \n",
    "        if self.stage == 'test':\n",
    "            def _load_function(idx):\n",
    "#                 idx = idx % len(self.dataset)\n",
    "                img0, class_label, dom_label = self.dataset[idx]\n",
    "                img0 = load_image(img0)\n",
    "                image = self.transform[self.stage](img0)\n",
    "               \n",
    "                class_label = torch.LongTensor(int(class_label))\n",
    "                dom_label = torch.LongTensor(int(dom_label))\n",
    "                \n",
    "#                 print('--------------------------')\n",
    "\n",
    "                return image, (class_label, dom_label)\n",
    "\n",
    "            def _collate_fun(batch):\n",
    "                return default_collate(batch)\n",
    "        \n",
    "        else:\n",
    "            def _load_function(idx):\n",
    "                idx = idx % len(self.dataset)\n",
    "                img0, class_label, dom_label = self.dataset[idx]\n",
    "\n",
    "                img0 = load_image(img0)\n",
    "                rotated_imgs = [\n",
    "                    self.transform[self.stage]['crop'](img0),\n",
    "                    self.transform[self.stage]['color'](img0),\n",
    "#                     transform(rotate_img(img0,  90)),\n",
    "#                     transform(rotate_img(img0, 180)),\n",
    "#                     transform(rotate_img(img0, 270))\n",
    "                ]\n",
    "                labels = [(int(class_label), int(dom_label)) for _ in range(2)]\n",
    "                rotation_labels = torch.LongTensor(labels)\n",
    "                return torch.stack(rotated_imgs, dim=0), rotation_labels\n",
    "\n",
    "            def _collate_fun(batch):\n",
    "                batch = default_collate(batch)\n",
    "                assert(len(batch)==2)\n",
    "#                 print('batch[1]', batch[1])\n",
    "                batch_size, rotations, channels, height, width = batch[0].size()\n",
    "                batch[0] = batch[0].view([batch_size*rotations, channels, height, width])\n",
    "                batch[1] = batch[1].view((batch_size*rotations, 2))\n",
    "                return batch\n",
    "\n",
    "\n",
    "        tnt_dataset = tnt.dataset.ListDataset(elem_list=range(self.epoch_size),\n",
    "            load=_load_function)\n",
    "        data_loader = tnt_dataset.parallel(batch_size=self.batch_size,\n",
    "            collate_fn=_collate_fun, num_workers=self.num_workers,\n",
    "            shuffle=self.shuffle)\n",
    "        return data_loader\n",
    "\n",
    "    def __call__(self, epoch=0):\n",
    "        return self.get_iterator(epoch)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.epoch_size / self.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 7137, 'val': 806, 'test': 2344}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader = {x: DataLoader_custom(GenericDataset(stage=x), stage=x) for x in ['train', 'val', 'test']}\n",
    "data_size = {x: len(GenericDataset(stage=x)) for x in ['train', 'val', 'test']}\n",
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "893"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataloader['train']())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataPrefetcher():\n",
    "    def __init__(self, loader):\n",
    "        self.loader = iter(loader)\n",
    "        self.stream = torch.cuda.Stream()\n",
    "        # With Amp, it isn't necessary to manually convert data to half.\n",
    "        # if args.fp16:\n",
    "        #     self.mean = self.mean.half()\n",
    "        #     self.std = self.std.half()\n",
    "        self.preload()\n",
    "\n",
    "    def preload(self):\n",
    "        try:\n",
    "            self.batch = next(self.loader)\n",
    "        except StopIteration:\n",
    "            self.batch = None\n",
    "            return\n",
    "        with torch.cuda.stream(self.stream):\n",
    "            for k in range(len(self.batch)):\n",
    "                if k != 'meta':\n",
    "                    self.batch[k] = self.batch[k].to(device=device, non_blocking=True)\n",
    "\n",
    "            # With Amp, it isn't necessary to manually convert data to half.\n",
    "            # if args.fp16:\n",
    "            #     self.next_input = self.next_input.half()\n",
    "            # else:\n",
    "            #     self.next_input = self.next_input.float()\n",
    "\n",
    "    def next(self):\n",
    "        torch.cuda.current_stream().wait_stream(self.stream)\n",
    "        batch = self.batch\n",
    "        self.preload()\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 224, 224])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefetcher = DataPrefetcher(dataloader['train']())\n",
    "image, label = prefetcher.next()\n",
    "\n",
    "image.size()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** check the dataloader order as above according to Paper: A Simple Framework for Contrastive Learning of Visual Representations\n",
    "\n",
    "**REFER:** [adverserial training](https://zhuanlan.zhihu.com/p/50710267)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_distance(k):\n",
    "    # k: [1,.., N]\n",
    "    return -torch.log(torch.exp(S[2*k - 1-1][2*k-1]) / torch.sum(torch.exp(S[2 *k-1-1]))\\\n",
    "+ torch.exp(S[2*k - 1][2*k - 1-1]) / torch.sum(torch.exp(S[2*k -1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO  alpha setting:** [refer](https://github.com/ShichengChen/Domain-Adversarial-Training-of-Neural-Networks/blob/master/mnist.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=50):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    dataloader = {x: DataLoader_custom(GenericDataset(stage=x), stage=x) for x in ['train', 'val', 'test']}\n",
    "    data_size = {x: len(GenericDataset(stage=x)) for x in ['train', 'val', 'test']}\n",
    "    best_acc = 0.0\n",
    "    alpha = 0.0\n",
    "\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs))\n",
    "        print('-' * 20)\n",
    "        \n",
    "        i = 0\n",
    "     \n",
    "        for phase in ['train', 'val']:\n",
    "            len_dataloader = len(dataloader[phase](epoch))\n",
    "            dom_data_size = data_size[phase]\n",
    "            if phase == 'train':\n",
    "                total_steps = num_epochs * len_dataloader\n",
    "                start_steps = epoch * len_dataloader\n",
    "                p = float(i + start_steps) / total_steps\n",
    "                alpha = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            ## train each dom, sum over all losses, and backward the total loss\n",
    "\n",
    "            doms_data_size = 0\n",
    "            # prefetcher = DataPrefetcher(dataloader['train']())\n",
    "            dom_data_loader = DataPrefetcher(dataloader[phase](epoch))\n",
    "            \n",
    "            \n",
    "            \n",
    "            batch = dom_data_loader.next()\n",
    "            while batch is not None:\n",
    "                inputs, labels  = batch\n",
    "\n",
    "#             for inputs, labels in dom_data_loader.next():\n",
    "#                 inputs = inputs.to(device)\n",
    "#                 labels = labels.to(device)\n",
    "\n",
    "                label_class = labels[:, 0]\n",
    "                label_dom = labels[:, 1]\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs, alpha) # 0: contrastive_feature, 1: class_label, 2: dom_label\n",
    "\n",
    "                    batch_data_size = inputs.size(0)\n",
    "                    \n",
    "                    k_len = batch_data_size / 2 + 1\n",
    "                    \n",
    "                    contrastive_features = outputs[0].view(batch_data_size, 1, 128)\n",
    "                    S = torch.Tensor([[torch.cosine_similarity(contrastive_features[i], contrastive_features[x])\\\n",
    "                                       for x in range(batch_data_size)] for i in range(batch_data_size)])\n",
    "\n",
    "                    contrastive_loss = 0\n",
    "                    for k in range(1, int(k_len)):\n",
    "                        contrastive_distance =  -torch.log(torch.exp(S[2*k - 1-1][2*k-1]) / torch.sum(torch.exp(S[2 *k-1-1]))\\\n",
    "+ torch.exp(S[2*k - 1][2*k - 1-1]) / torch.sum(torch.exp(S[2*k -1])))\n",
    "                        contrastive_loss += contrastive_distance\n",
    "\n",
    "                    contrastive_loss = contrastive_loss.to(device)\n",
    "                    loss_class = criterion(outputs[1], label_class)\n",
    "                    loss_dom = criterion(outputs[2], label_dom)\n",
    "\n",
    "                    total_loss = 0.6 * (contrastive_loss + loss_class) + 0.4 * loss_dom\n",
    "\n",
    "                    _, preds_classes = torch.max(outputs[1], 1)\n",
    "                    _, preds_dom = torch.max(outputs[2], 1)\n",
    "\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        i += 1\n",
    "                        total_loss.backward()\n",
    "                        optimizer.step()\n",
    "                    batch = dom_data_loader.next()\n",
    "\n",
    "                running_loss += total_loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds_classes == label_class.data)\n",
    "            \n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "\n",
    "            epoch_loss = running_loss / dom_data_size / 2\n",
    "            epoch_acc = running_corrects.double() / dom_data_size / 2\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "\n",
    "\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "\n",
    "    if not os.path.exists('./models'):\n",
    "        os.makedirs('./models')\n",
    "\n",
    "    outfile = os.path.join('./models', 'best_model_adversarial_contrastive_0.4.tar')\n",
    "    torch.save({'state': best_model_wts, 'alpha': alpha}, outfile)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = nn.DataParallel(model, device_ids=[0,1,2,3])\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Observe that all parameters are being optimized\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20\n",
      "--------------------\n",
      "train Loss: 10.6327 Acc: 0.5042\n",
      "val Loss: 9.8574 Acc: 0.7128\n",
      "Epoch 1/20\n",
      "--------------------\n",
      "train Loss: 10.3828 Acc: 0.6580\n",
      "val Loss: 9.7494 Acc: 0.7854\n",
      "Epoch 2/20\n",
      "--------------------\n",
      "train Loss: 10.2896 Acc: 0.7136\n",
      "val Loss: 9.8828 Acc: 0.7723\n",
      "Epoch 3/20\n",
      "--------------------\n",
      "train Loss: 10.2342 Acc: 0.7456\n",
      "val Loss: 9.8158 Acc: 0.7860\n",
      "Epoch 4/20\n",
      "--------------------\n",
      "train Loss: 10.2271 Acc: 0.7713\n",
      "val Loss: 9.7671 Acc: 0.8313\n",
      "Epoch 5/20\n",
      "--------------------\n",
      "train Loss: 10.2099 Acc: 0.7927\n",
      "val Loss: 9.8347 Acc: 0.8220\n",
      "Epoch 6/20\n",
      "--------------------\n",
      "train Loss: 10.2038 Acc: 0.8064\n",
      "val Loss: 9.7708 Acc: 0.8412\n",
      "Epoch 7/20\n",
      "--------------------\n",
      "train Loss: 10.1053 Acc: 0.8649\n",
      "val Loss: 9.6667 Acc: 0.8989\n",
      "Epoch 8/20\n",
      "--------------------\n",
      "train Loss: 10.0582 Acc: 0.8854\n",
      "val Loss: 9.6605 Acc: 0.9119\n",
      "Epoch 9/20\n",
      "--------------------\n",
      "train Loss: 10.0370 Acc: 0.8936\n",
      "val Loss: 9.6501 Acc: 0.9076\n",
      "Epoch 10/20\n",
      "--------------------\n",
      "train Loss: 10.0195 Acc: 0.9002\n",
      "val Loss: 9.6705 Acc: 0.9026\n",
      "Epoch 11/20\n",
      "--------------------\n",
      "train Loss: 10.0146 Acc: 0.9049\n",
      "val Loss: 9.7084 Acc: 0.9014\n",
      "Epoch 12/20\n",
      "--------------------\n",
      "train Loss: 10.0027 Acc: 0.9132\n",
      "val Loss: 9.6982 Acc: 0.8970\n",
      "Epoch 13/20\n",
      "--------------------\n",
      "train Loss: 9.9863 Acc: 0.9161\n",
      "val Loss: 9.6726 Acc: 0.9069\n",
      "Epoch 14/20\n",
      "--------------------\n",
      "train Loss: 9.9955 Acc: 0.9175\n",
      "val Loss: 9.6772 Acc: 0.9113\n",
      "Epoch 15/20\n",
      "--------------------\n",
      "train Loss: 9.9896 Acc: 0.9225\n",
      "val Loss: 9.6778 Acc: 0.9082\n",
      "Epoch 16/20\n",
      "--------------------\n",
      "train Loss: 9.9889 Acc: 0.9222\n",
      "val Loss: 9.6731 Acc: 0.9088\n",
      "Epoch 17/20\n",
      "--------------------\n",
      "train Loss: 9.9933 Acc: 0.9229\n",
      "val Loss: 9.6753 Acc: 0.9076\n",
      "Epoch 18/20\n",
      "--------------------\n",
      "train Loss: 9.9958 Acc: 0.9203\n",
      "val Loss: 9.6839 Acc: 0.9206\n",
      "Epoch 19/20\n",
      "--------------------\n",
      "train Loss: 10.0049 Acc: 0.9242\n",
      "val Loss: 9.6532 Acc: 0.9076\n",
      "Best val Acc: 0.920596\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dom_dataset = datasets.ImageFolder(osp.join(data_dir, 'art_painting', 'test'), transforms.Compose([\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dom_dataloader = DataLoader(test_dom_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuarcy  tensor(10, device='cuda:3')\n",
      "Accuarcy  tensor(21, device='cuda:3')\n",
      "Accuarcy  tensor(34, device='cuda:3')\n",
      "Accuarcy  tensor(44, device='cuda:3')\n",
      "Accuarcy  tensor(55, device='cuda:3')\n",
      "Accuarcy  tensor(68, device='cuda:3')\n",
      "Accuarcy  tensor(78, device='cuda:3')\n",
      "Accuarcy  tensor(87, device='cuda:3')\n",
      "Accuarcy  tensor(95, device='cuda:3')\n",
      "Accuarcy  tensor(105, device='cuda:3')\n",
      "Accuarcy  tensor(115, device='cuda:3')\n",
      "Accuarcy  tensor(122, device='cuda:3')\n",
      "Accuarcy  tensor(129, device='cuda:3')\n",
      "Accuarcy  tensor(136, device='cuda:3')\n",
      "Accuarcy  tensor(147, device='cuda:3')\n",
      "Accuarcy  tensor(156, device='cuda:3')\n",
      "Accuarcy  tensor(165, device='cuda:3')\n",
      "Accuarcy  tensor(173, device='cuda:3')\n",
      "Accuarcy  tensor(181, device='cuda:3')\n",
      "Accuarcy  tensor(189, device='cuda:3')\n",
      "Accuarcy  tensor(195, device='cuda:3')\n",
      "Accuarcy  tensor(202, device='cuda:3')\n",
      "Accuarcy  tensor(212, device='cuda:3')\n",
      "Accuarcy  tensor(220, device='cuda:3')\n",
      "Accuarcy  tensor(233, device='cuda:3')\n",
      "Accuarcy  tensor(242, device='cuda:3')\n",
      "Accuarcy  tensor(253, device='cuda:3')\n",
      "Accuarcy  tensor(267, device='cuda:3')\n",
      "Accuarcy  tensor(280, device='cuda:3')\n",
      "Accuarcy  tensor(294, device='cuda:3')\n",
      "Accuarcy  tensor(308, device='cuda:3')\n",
      "Accuarcy  tensor(321, device='cuda:3')\n",
      "Accuarcy  tensor(331, device='cuda:3')\n",
      "Accuarcy  tensor(343, device='cuda:3')\n",
      "Accuarcy  tensor(355, device='cuda:3')\n",
      "Accuarcy  tensor(370, device='cuda:3')\n",
      "Accuarcy  tensor(384, device='cuda:3')\n",
      "Accuarcy  tensor(397, device='cuda:3')\n",
      "Accuarcy  tensor(411, device='cuda:3')\n",
      "Accuarcy  tensor(426, device='cuda:3')\n",
      "Accuarcy  tensor(439, device='cuda:3')\n",
      "Accuarcy  tensor(454, device='cuda:3')\n",
      "Accuarcy  tensor(464, device='cuda:3')\n",
      "Accuarcy  tensor(476, device='cuda:3')\n",
      "Accuarcy  tensor(485, device='cuda:3')\n",
      "Accuarcy  tensor(495, device='cuda:3')\n",
      "Accuarcy  tensor(506, device='cuda:3')\n",
      "Accuarcy  tensor(518, device='cuda:3')\n",
      "Accuarcy  tensor(530, device='cuda:3')\n",
      "Accuarcy  tensor(541, device='cuda:3')\n",
      "Accuarcy  tensor(552, device='cuda:3')\n",
      "Accuarcy  tensor(562, device='cuda:3')\n",
      "Accuarcy  tensor(570, device='cuda:3')\n",
      "Accuarcy  tensor(580, device='cuda:3')\n",
      "Accuarcy  tensor(588, device='cuda:3')\n",
      "Accuarcy  tensor(600, device='cuda:3')\n",
      "Accuarcy  tensor(611, device='cuda:3')\n",
      "Accuarcy  tensor(620, device='cuda:3')\n",
      "Accuarcy  tensor(631, device='cuda:3')\n",
      "Accuarcy  tensor(643, device='cuda:3')\n",
      "Accuarcy  tensor(655, device='cuda:3')\n",
      "Accuarcy  tensor(669, device='cuda:3')\n",
      "Accuarcy  tensor(681, device='cuda:3')\n",
      "Accuarcy  tensor(694, device='cuda:3')\n",
      "Accuarcy  tensor(706, device='cuda:3')\n",
      "Accuarcy  tensor(713, device='cuda:3')\n",
      "Accuarcy  tensor(723, device='cuda:3')\n",
      "Accuarcy  tensor(734, device='cuda:3')\n",
      "Accuarcy  tensor(744, device='cuda:3')\n",
      "Accuarcy  tensor(759, device='cuda:3')\n",
      "Accuarcy  tensor(768, device='cuda:3')\n",
      "Accuarcy  tensor(779, device='cuda:3')\n",
      "Accuarcy  tensor(791, device='cuda:3')\n",
      "Accuarcy  tensor(805, device='cuda:3')\n",
      "Accuarcy  tensor(819, device='cuda:3')\n",
      "Accuarcy  tensor(827, device='cuda:3')\n",
      "Accuarcy  tensor(838, device='cuda:3')\n",
      "Accuarcy  tensor(850, device='cuda:3')\n",
      "Accuarcy  tensor(865, device='cuda:3')\n",
      "Accuarcy  tensor(875, device='cuda:3')\n",
      "Accuarcy  tensor(888, device='cuda:3')\n",
      "Accuarcy  tensor(899, device='cuda:3')\n",
      "Accuarcy  tensor(908, device='cuda:3')\n",
      "Accuarcy  tensor(919, device='cuda:3')\n",
      "Accuarcy  tensor(929, device='cuda:3')\n",
      "Accuarcy  tensor(938, device='cuda:3')\n",
      "Accuarcy  tensor(949, device='cuda:3')\n",
      "Accuarcy  tensor(958, device='cuda:3')\n",
      "Accuarcy  tensor(971, device='cuda:3')\n",
      "Accuarcy  tensor(982, device='cuda:3')\n",
      "Accuarcy  tensor(995, device='cuda:3')\n",
      "Accuarcy  tensor(1006, device='cuda:3')\n",
      "Accuarcy  tensor(1018, device='cuda:3')\n",
      "Accuarcy  tensor(1025, device='cuda:3')\n",
      "Accuarcy  tensor(1035, device='cuda:3')\n",
      "Accuarcy  tensor(1044, device='cuda:3')\n",
      "Accuarcy  tensor(1052, device='cuda:3')\n",
      "Accuarcy  tensor(1062, device='cuda:3')\n",
      "Accuarcy  tensor(1071, device='cuda:3')\n",
      "Accuarcy  tensor(1077, device='cuda:3')\n",
      "Accuarcy  tensor(1084, device='cuda:3')\n",
      "Accuarcy  tensor(1090, device='cuda:3')\n",
      "Accuarcy  tensor(1093, device='cuda:3')\n",
      "Accuarcy  tensor(1101, device='cuda:3')\n",
      "Accuarcy  tensor(1109, device='cuda:3')\n",
      "Accuarcy  tensor(1117, device='cuda:3')\n",
      "Accuarcy  tensor(1124, device='cuda:3')\n",
      "Accuarcy  tensor(1129, device='cuda:3')\n",
      "Accuarcy  tensor(1137, device='cuda:3')\n",
      "Accuarcy  tensor(1149, device='cuda:3')\n",
      "Accuarcy  tensor(1153, device='cuda:3')\n",
      "Accuarcy  tensor(1161, device='cuda:3')\n",
      "Accuarcy  tensor(1167, device='cuda:3')\n",
      "Accuarcy  tensor(1173, device='cuda:3')\n",
      "Accuarcy  tensor(1180, device='cuda:3')\n",
      "Accuarcy  tensor(1185, device='cuda:3')\n",
      "Accuarcy  tensor(1188, device='cuda:3')\n",
      "Accuarcy  tensor(1194, device='cuda:3')\n",
      "Accuarcy  tensor(1197, device='cuda:3')\n",
      "Accuarcy  tensor(1203, device='cuda:3')\n",
      "Accuarcy  tensor(1206, device='cuda:3')\n",
      "Accuarcy  tensor(1209, device='cuda:3')\n",
      "Accuarcy  tensor(1214, device='cuda:3')\n",
      "Accuarcy  tensor(1221, device='cuda:3')\n",
      "Accuarcy  tensor(1226, device='cuda:3')\n",
      "Accuarcy  tensor(1232, device='cuda:3')\n",
      "Accuarcy  tensor(1237, device='cuda:3')\n",
      "Accuarcy  tensor(1238, device='cuda:3')\n",
      "test on art domain, accuracy is:  tensor(1238, device='cuda:3')\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "for images, labels in test_dom_dataloader:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    \n",
    "    outputs = model(images, 0.8)[1]\n",
    "    _, preds_classes = torch.max(outputs, 1)\n",
    "    \n",
    "    acc += torch.sum(preds_classes == labels.data)\n",
    "    print('Accuarcy ', acc)\n",
    "print('test on art domain, accuracy is: ', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi GPU train, to load saved weights\n",
    "def load_network(network):\n",
    "    save_path = os.path.join('./model',name,'net_%s.pth'%opt.which_epoch)\n",
    "    state_dict = torch.load(save_path)\n",
    "    # create new OrderedDict that does not contain `module.`\n",
    "    from collections import OrderedDict\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        namekey = k[7:] # remove `module.`\n",
    "        new_state_dict[namekey] = v\n",
    "    # load params\n",
    "    network.load_state_dict(new_state_dict)\n",
    "    return network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:**\n",
    "mutilple GPU train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
